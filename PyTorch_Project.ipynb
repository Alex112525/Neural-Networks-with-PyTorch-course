{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO95NF8fPtQd8Wd4EaeX3l0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex112525/Neural-Networks-with-PyTorch-course/blob/main/PyTorch_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TorchText* is a powerful library that can be used for many natural language processing (NLP) tasks. Some of the use cases for TorchText include text classification, sequence tagging, machine translation, and sentiment analysis"
      ],
      "metadata": {
        "id": "d8vpPdsC7vXm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjbW3EAd7DWh",
        "outputId": "1984f1ea-a8f1-4a4b-e78e-87c3a890a541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.22.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (1.26.15)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker>=2.0.0\n",
        "!pip install torchtext --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import DBpedia\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "torchtext.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Jn_cdQmB72LA",
        "outputId": "ef17ae92-c0bb-4330-e805-d61b3043bf56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.15.2+cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset processing and vocabulary building"
      ],
      "metadata": {
        "id": "wO6niOU68a29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_DB = iter(DBpedia(split=\"train\"))"
      ],
      "metadata": {
        "id": "gNSqzzZL8NrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(test_DB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfyw5_dN85tZ",
        "outputId": "d1dae5a7-5112-4de9-c7bf-7a2bda81f7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'E. D. Abbott Ltd  Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types â€“ *word*, *character*, and *subword* (n-gram characters)"
      ],
      "metadata": {
        "id": "1g29dT1E9O9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = DBpedia(split=\"train\")\n",
        "\n",
        "def yield_tokens(data_iter: iter) -> list:\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unknown>\"])\n",
        "vocab.set_default_index(vocab[\"<unknown>\"])"
      ],
      "metadata": {
        "id": "5SCWGA2V9EAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = tokenizer(\"Hello I am Alex, I'm studying in Platzi\")\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWw3zR_O-13Q",
        "outputId": "dcf9db3d-575f-4b1d-e6d3-db5b200a2406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'i', 'am', 'alex', ',', 'i', \"'\", 'm', 'studying', 'in', 'platzi']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JogR2eIX_Rdc",
        "outputId": "1422d16a-601e-4f10-d213-6ee4421562ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7296, 187, 2409, 2215, 90515, 187, 17, 104, 4782, 3, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "metadata": {
        "id": "dCsL7Va2_WLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline(\"Hello I'm Alex\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0meV0lsZ_mTY",
        "outputId": "44a08eca-1b41-41c6-9893-be85447481b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7296, 187, 17, 104, 2215]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a Dataloader"
      ],
      "metadata": {
        "id": "X_yeiuJz_60I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch: list) -> tuple:\n",
        "  label_list = []\n",
        "  text_list = []\n",
        "  offsets = [0]\n",
        "\n",
        "  for (_label, _text) in batch:\n",
        "    label_list.append(label_pipeline(_label))\n",
        "    processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "    offsets.append(processed_text.size(0))\n",
        "\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text_list = torch.cat(text_list)\n",
        "\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "wC4q56fLBjss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A *dataloader* is a PyTorch utility that helps in loading and batching data. It provides an iterable over a dataset that yields batches of data. The *dataloader* can also handle batching, shuffling, and multiprocess data loading.\n",
        "\n",
        "Dataloaders provide a number of benefits such as:\n",
        "\n",
        "- **Efficient data loading**: Dataloaders can load data in parallel using multiprocessing workers. \n",
        "- **Batching**: Dataloaders can batch data together, which is useful for training deep learning models. \n",
        "- **Shuffling**: Dataloaders can shuffle the data before each epoch. This is useful for training deep learning models because it helps prevent overfitting.\n",
        "- **Customization**: Dataloaders are highly customizable. You can define your own collate function to control how samples are batched together, and you can define your own sampler to control how samples are selected from the dataset."
      ],
      "metadata": {
        "id": "-OZMvhIhARzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = DBpedia(split=\"train\")\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "mfzgsNtYAWhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (label, text, offset) in enumerate(dataloader):\n",
        "    print(f\"label: {label}, text: {text}, offset: {offset}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKISIZZm_560",
        "outputId": "95675466-9e8c-46f4-d50e-9a4e1481c481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), text: tensor([184535, 184535,    453,      2,      6,      5,     50,  17634,   1804,\n",
            "             4,   1545,   4034,   3905,    684,      3,    287,    906,    153,\n",
            "             2,      1,     54,    845,      3,   2752,     55,   1004,    153,\n",
            "           910,   2071,    390,    886,     42,    705,     42,    120,    383,\n",
            "           713,      7,   1159,      2, 184535,     10,   5820,      3,    121,\n",
            "           193,     11,   9501, 344439,   2617,  78806,      7,   3268, 220285,\n",
            "             2,  78806,      7, 220285,   2576,    445,     22,   5605,   1129,\n",
            "            21,      4,    334,      7,    161,   1720,     14,      1,    826,\n",
            "            17,     18, 132749,     17,     18,      2,  51445,   4674,   8618,\n",
            "         51445,      6,     19,   4674,   3038,   1112,      7,    305,     54,\n",
            "            40,     10,    212,      3,   2379,    390,      2,     98,      3,\n",
            "           647,     11,   3299,   4727,      5,     42,    225,    181,      1,\n",
            "            54,     32,     58,      5,   4571,     16,   1031, 544027,  15690,\n",
            "         48746,      7,  42817,      2,    584,   4447,    314,      1,    584,\n",
            "          4447,    314,      8,    308, 783233, 516492, 105883,    243, 577516,\n",
            "             8, 796701,      9,   1034, 516219, 801936,      8, 440510,      9,\n",
            "          4862, 202029, 640479,  49156,      8, 440510,      9,      9,      6,\n",
            "             5,    525,      4, 204338,   4847, 222232,    231,   4494,      3,\n",
            "           368,     89,      1,    507,   5079,      3,  43283,      8, 790393,\n",
            "             9,      7,    373,   1974,    134,     29,   1145,    565,     11,\n",
            "             1, 204338,   1165, 431529, 599117,      8, 789370,  60050,     71,\n",
            "         39475,      9,      2,      1,    106,    392, 431529,     17,     18,\n",
            "        129746,      7,   6767,     15, 734664, 739180,     17,     18,      8,\n",
            "        800236,  18553,     71,  90533,      9,    525,    704,  13386,     15,\n",
            "           473,   5763, 783236,   1727,      5,    664,      4,    704, 222232,\n",
            "             2, 229008, 229008,      6,     19,   2841,    771,     54,     48,\n",
            "           791,    771,     40,   5010,    901,      7,   5270,     12,  34123,\n",
            "        229994,    611,  21062,   8921,      2,      1,   7742,    352,    478,\n",
            "           771,     40,   5010,   3771,     12,   2646,    962,    611,  17853,\n",
            "             7, 725406,      5,   6964,   6340,     12,  12910,  64712,   2247,\n",
            "          3045,     44, 167453,    962,   3538,   3107,    152,      1,    665,\n",
            "             4, 156388,      2, 229008,    338,      3,    193,      7,      6,\n",
            "           684,      3,   1886,    381,      2,   7322,  22207,    105, 313844,\n",
            "             1,   7322,  22207,    105, 313844,      8, 399413,      9,      6,\n",
            "             5,   1983,     54,    360,     11,      1,    382,      4,    768,\n",
            "             2,     29,   1413,      6,     12,  13590,   1786,     12,    768,\n",
            "            11,  13921,      3,    149,    901,      2,      1,     54,      6,\n",
            "          4662,     11,   4390,  42139,     44,      6,   1433,     12,   9375,\n",
            "         95624,    810,      4,      1,  12234,  10164,      2,  24683,    560,\n",
            "         24683,    560,    453,      2,      8,    461,     35,     14,    905,\n",
            "         19718,    453,      7,   3772,     35,     14,  24683,      9,      6,\n",
            "             5,    181,   1262,    560,     54,     40,    161,     32,    582,\n",
            "           295,   4533,   2975,  22035,   2157,      1,  30340,  66077, 298073,\n",
            "         22486,      7, 108041,      2,      5,   3009,  24683,      2,   1776,\n",
            "             8,    387,  43600,    757,   1833,      9,    338,     15,    136,\n",
            "           170,    812,      2,    157,  24683,    560,   2563,     47,     58,\n",
            "            15,  10566,     29,   9935,   1262,    830,   1702,      2,  24683,\n",
            "             6,    684,    228,  10948,   2941,    381,    268,      2,    226,\n",
            "             7,    689,     20,   7268,    256,      3,     42,    120,     66,\n",
            "             2,    281,    614,     16,   1937,   1372,      1,    281,    614,\n",
            "            16,   1937,   1372,      6,      5,   7196,    614,     80,      3,\n",
            "           311,      8,     50,    589,      9,     40,     10,     98,      3,\n",
            "          2020,     14,      1,     93,   1937,    614,     12,   3924,    937,\n",
            "          4131,      7,   7325,      4,     93,    658,      7,   1660,   1025,\n",
            "             2,      1,   8798,   9610,      4,      1,    614,     17,     18,\n",
            "          2401,     32,    143,   1974,     12,    478,      1,   3306,      4,\n",
            "           658,      2,   1349,    119,   1299,   1579,      1,    614,     17,\n",
            "            18,    240,   1937,   1372,     48,     32,     90,      3,   2079,\n",
            "           143,   1364,      2, 343468,    441,      1, 343468,    441,      8,\n",
            "          6530, 603791,      9,      6,      5,   7900,  47318,   2661,    766,\n",
            "           146,     35,     16,     29,   5387,   4260,     16,   1311,   2661,\n",
            "             2], device='cuda:0'), offset: tensor([  0,  78, 121, 226, 293, 349, 442, 525], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating architecture for the classification model"
      ],
      "metadata": {
        "id": "XF-meaI3EDjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "  def __init__(self, vocab_size: int, embed_dim: int, num_class: int) -> None:\n",
        "    super(ClassificationModel, self).__init__()\n",
        "\n",
        "    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
        "    self.bn1 = nn.BatchNorm1d(embed_dim)\n",
        "    self.fc = nn.Linear(embed_dim, num_class)\n",
        "\n",
        "  def forward(self, text: torch.Tensor, offsets: torch.Tensor) -> torch.Tensor:\n",
        "    embedded = self.embedding(text, offsets)\n",
        "    embedded_norm = self.bn1(embedded)\n",
        "    embedded_activated = F.relu(embedded_norm)\n",
        "\n",
        "    return self.fc(embedded_activated)"
      ],
      "metadata": {
        "id": "kE-f0uxCD9fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = DBpedia(split=\"train\")\n",
        "num_class = len(set([label for (label, _) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "embedding = 128\n",
        "\n",
        "model_cm = ClassificationModel(vocab_size=vocab_size, embed_dim=embedding, num_class=num_class).to(device)"
      ],
      "metadata": {
        "id": "VEBRvj7dF8j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoNSX2lwItFN",
        "outputId": "bf9941e8-60e0-4685-8ec7-51d0654ea542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassificationModel(\n",
              "  (embedding): EmbeddingBag(802998, 128, mode='mean')\n",
              "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc): Linear(in_features=128, out_features=14, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cm(text, offset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMWkuf7VUfIa",
        "outputId": "ff6762c3-9188-4950-9513-93b07ec7e6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.2848e-01,  3.2775e-01,  2.3615e-02, -4.9947e-01,  2.7661e-01,\n",
              "         -1.7258e-01, -4.8254e-01,  2.8958e-01,  4.3761e-02, -3.3177e-01,\n",
              "          2.8847e-01, -2.8055e-01,  1.4088e-01,  1.3044e-01],\n",
              "        [-6.9019e-01, -3.8628e-01, -2.7909e-01, -6.5109e-01, -2.3605e-03,\n",
              "         -5.6130e-03,  5.3470e-01, -8.6261e-02, -2.0238e-01, -5.9136e-01,\n",
              "          4.6033e-01,  3.1968e-02, -5.2840e-01, -5.3199e-01],\n",
              "        [ 1.1558e-01, -5.7493e-02,  1.1665e-03, -6.1120e-01,  2.3265e-01,\n",
              "          5.3019e-01,  4.2965e-01, -2.2074e-01,  7.4407e-02,  1.0521e-01,\n",
              "          1.5437e-01, -5.0819e-01, -3.1330e-01, -4.4089e-01],\n",
              "        [ 5.2036e-01,  3.2213e-01,  1.5288e-01, -4.9221e-01, -3.0451e-03,\n",
              "         -4.8278e-02,  1.1372e-01, -5.6361e-01, -2.5408e-01, -7.9610e-01,\n",
              "         -1.2549e-01, -3.0932e-01, -8.1546e-01,  4.6232e-01],\n",
              "        [ 5.9216e-01, -3.4557e-01,  3.1020e-01, -9.5563e-01,  3.1538e-01,\n",
              "         -1.2268e-01,  5.7765e-02,  4.0617e-02,  3.1634e-01, -1.4622e+00,\n",
              "         -2.3018e-01, -5.9942e-01, -6.5296e-02,  6.4317e-03],\n",
              "        [ 2.9487e-01,  2.1158e-01, -1.7679e-01, -2.7067e-01,  5.3379e-01,\n",
              "         -2.1953e-01, -2.8086e-01,  3.7649e-01,  4.3840e-01, -4.6503e-01,\n",
              "          2.5394e-01,  2.3159e-01, -5.4037e-01, -8.3012e-02],\n",
              "        [-1.9920e-01, -2.5837e-01,  4.8105e-01, -1.0756e+00, -3.5662e-02,\n",
              "          3.4302e-01,  6.3459e-02, -2.9464e-02,  8.6742e-01, -3.0969e-01,\n",
              "          2.3966e-02, -8.0853e-01, -8.0400e-02,  2.8371e-02],\n",
              "        [ 1.9000e-01, -1.9120e-02,  4.6476e-01, -3.4773e-02,  3.8407e-01,\n",
              "          4.5952e-01, -5.2545e-02,  2.2794e-01,  7.4861e-01, -7.2072e-01,\n",
              "          1.2417e+00, -1.2392e+00,  2.4946e-01, -5.7001e-01]], device='cuda:0',\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*numel()* is a PyTorch function that returns the total number of elements in the input tensor"
      ],
      "metadata": {
        "id": "10CyABoGJTIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model: ClassificationModel) -> int:\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "q2hvFePGI3N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The model have {count_parameters(model_cm):,} trainable parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_ZLBWisJV_m",
        "outputId": "a6a612c7-70bd-4bd4-e442-b02665bca0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model have 102,785,806 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ],
      "metadata": {
        "id": "x6aJD5W5J66p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *utils.clip_grad_norm_* function is used to avoid the \"exploding gradient\" problem in neural network training. This function is responsible for normalizing the gradients of the model parameters so that they are not too large. If the gradients are too large, they can cause the model to not converge or even diverge. \n",
        "\n",
        "The function *clip_grad_norm_* takes as input the model parameters and a maximum value for the gradient norm. If the gradient norm is greater than the maximum value, then the gradients are normalized to have a norm equal to the maximum value."
      ],
      "metadata": {
        "id": "G4ZhAkrbmkM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: ClassificationModel, dataloader: DataLoader) -> tuple:\n",
        "  model.train()\n",
        "\n",
        "  epoch_acc = 0\n",
        "  epoch_loss = 0\n",
        "  total_count = 0\n",
        "\n",
        "  for i, (label, text, offset) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predict = model(text, offset)\n",
        "\n",
        "    loss = loss_fn(predict, label)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    acc = (predict.argmax(1) == label).sum()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_acc += acc.item()\n",
        "    epoch_loss += loss.item()\n",
        "    total_count += label.size(0)\n",
        "\n",
        "    if i%500 == 0:\n",
        "      print(f\"epoch:{epoch} | {i}/{len(dataloader)} batches| loss:{epoch_loss/total_count} | acc:{epoch_acc/total_count}\")\n",
        "    \n",
        "  return epoch_acc/total_count, epoch_loss/total_count\n"
      ],
      "metadata": {
        "id": "aLGUDWSTJ9UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model: ClassificationModel, dataloader: DataLoader) -> tuple:\n",
        "  model.eval()\n",
        "  epoch_acc = 0\n",
        "  total_count = 0\n",
        "  epoch_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (label, text, offset) in enumerate(dataloader):\n",
        "      predict = model(text, offset)\n",
        "\n",
        "      loss = loss_fn(predict, label)\n",
        "      acc = (predict.argmax(1) == label).sum()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "      total_count += label.size(0)\n",
        "\n",
        "  return epoch_acc/total_count, epoch_loss/total_count"
      ],
      "metadata": {
        "id": "2ehoK_nEzyP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing training: data splitting, loss and optimization."
      ],
      "metadata": {
        "id": "XtFHiNWR26Fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, *hyperparameters* are parameters that are not learned from data, but are set prior to training. They are used to control the learning process and can have a significant impact on the performance of the model."
      ],
      "metadata": {
        "id": "HkrgO0eO618n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "EPOCH = 4\n",
        "LEARNING_RATE = 0.22\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "scMis2LQ2qFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_cm.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "tCkFidVg2va0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *to_map_style_dataset* function is used to convert a dataset into a map-style dataset. It takes an iterable dataset and returns a map-style dataset. \n",
        "\n",
        "The map-style dataset is a PyTorch Dataset that returns a dictionary of samples instead of a tuple of samples. Each sample in the dictionary is identified by a key. The keys are specified using the field_names argument. The *to_map_style_dataset* function is useful when you want to use a dataset with a DataLoader that requires a map-style dataset."
      ],
      "metadata": {
        "id": "_pnAagza5Ayn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = DBpedia()\n",
        "train_data = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "split_train = int(len(train_data)* 0.95)\n",
        "\n",
        "train_dataset, validation_dataset = random_split(train_data, [split_train, len(train_data)-split_train]) \n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "CCVSjlUw4bIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Evaluation."
      ],
      "metadata": {
        "id": "l4dcFEVF7AP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "major_loss_validation = float(\"inf\")\n",
        "\n",
        "for epoch in range(1, EPOCH+1):\n",
        "  train_acc, train_loss = train(model_cm, train_dataloader)\n",
        "\n",
        "  eval_acc, eval_loss = eval(model_cm, validation_dataloader)\n",
        "\n",
        "  if eval_loss < major_loss_validation:\n",
        "    best_valid_loss = eval_loss\n",
        "    torch.save(model_cm.state_dict(), \"best_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRZVNOoS7M7U",
        "outputId": "e27292bd-0f22-434e-f1c7-21370b4a55d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1 | 0/8313 batches| loss:0.021196749061346054 | acc:0.5625\n",
            "epoch:1 | 500/8313 batches| loss:0.018923175156845187 | acc:0.6487337824351297\n",
            "epoch:1 | 1000/8313 batches| loss:0.018162808962516135 | acc:0.6593874875124875\n",
            "epoch:1 | 1500/8313 batches| loss:0.017615180856596024 | acc:0.6672426715522984\n",
            "epoch:1 | 2000/8313 batches| loss:0.017196083099286684 | acc:0.6730775237381309\n",
            "epoch:1 | 2500/8313 batches| loss:0.0168769058746482 | acc:0.6771603858456617\n",
            "epoch:1 | 3000/8313 batches| loss:0.01656188335771195 | acc:0.6816061312895701\n",
            "epoch:1 | 3500/8313 batches| loss:0.016331112985354224 | acc:0.6852372536418166\n",
            "epoch:1 | 4000/8313 batches| loss:0.01610981972417446 | acc:0.6883396338415396\n",
            "epoch:1 | 4500/8313 batches| loss:0.01590883541749606 | acc:0.6913463674738947\n",
            "epoch:1 | 5000/8313 batches| loss:0.015709996309952273 | acc:0.6944267396520696\n",
            "epoch:1 | 5500/8313 batches| loss:0.015560685703776394 | acc:0.696575054535539\n",
            "epoch:1 | 6000/8313 batches| loss:0.01540673974446097 | acc:0.6991412889518414\n",
            "epoch:1 | 6500/8313 batches| loss:0.015266432312907847 | acc:0.7012022188894016\n",
            "epoch:1 | 7000/8313 batches| loss:0.015144211955598922 | acc:0.7031205363519497\n",
            "epoch:1 | 7500/8313 batches| loss:0.015029834464704923 | acc:0.7050539094787361\n",
            "epoch:1 | 8000/8313 batches| loss:0.014922494068101593 | acc:0.7066772903387076\n",
            "epoch:2 | 0/8313 batches| loss:0.01554682943969965 | acc:0.71875\n",
            "epoch:2 | 500/8313 batches| loss:0.012804934741955199 | acc:0.7418288423153693\n",
            "epoch:2 | 1000/8313 batches| loss:0.01277786127657264 | acc:0.742647977022977\n",
            "epoch:2 | 1500/8313 batches| loss:0.012734982850827987 | acc:0.7436708860759493\n",
            "epoch:2 | 2000/8313 batches| loss:0.01264657257571593 | acc:0.7454553973013494\n",
            "epoch:2 | 2500/8313 batches| loss:0.012631829190136837 | acc:0.7461890243902439\n",
            "epoch:2 | 3000/8313 batches| loss:0.012585995496480028 | acc:0.7471155448183938\n",
            "epoch:2 | 3500/8313 batches| loss:0.012530584039256483 | acc:0.7483129820051414\n",
            "epoch:2 | 4000/8313 batches| loss:0.012464038043296195 | acc:0.7498086415896026\n",
            "epoch:2 | 4500/8313 batches| loss:0.012394620951491048 | acc:0.7513781659631193\n",
            "epoch:2 | 5000/8313 batches| loss:0.012348266114800919 | acc:0.7523089132173565\n",
            "epoch:2 | 5500/8313 batches| loss:0.012288320241175989 | acc:0.7534936829667334\n",
            "epoch:2 | 6000/8313 batches| loss:0.012253126308136962 | acc:0.7540826528911848\n",
            "epoch:2 | 6500/8313 batches| loss:0.012207556514714387 | acc:0.7548454083987078\n",
            "epoch:2 | 7000/8313 batches| loss:0.012158237311535713 | acc:0.7557313240965576\n",
            "epoch:2 | 7500/8313 batches| loss:0.01211104593717119 | acc:0.7567345187308359\n",
            "epoch:2 | 8000/8313 batches| loss:0.012060648882222661 | acc:0.7577236439195101\n",
            "epoch:3 | 0/8313 batches| loss:0.009112837724387646 | acc:0.890625\n",
            "epoch:3 | 500/8313 batches| loss:0.011267307893876841 | acc:0.7750124750499002\n",
            "epoch:3 | 1000/8313 batches| loss:0.011220880267726673 | acc:0.7743506493506493\n",
            "epoch:3 | 1500/8313 batches| loss:0.01115619636380885 | acc:0.7757744836775483\n",
            "epoch:3 | 2000/8313 batches| loss:0.011038650407864042 | acc:0.7781812218890555\n",
            "epoch:3 | 2500/8313 batches| loss:0.01100379152924287 | acc:0.7782574470211915\n",
            "epoch:3 | 3000/8313 batches| loss:0.010973805236893619 | acc:0.7788653782072642\n",
            "epoch:3 | 3500/8313 batches| loss:0.010975048999267241 | acc:0.7787328620394173\n",
            "epoch:3 | 4000/8313 batches| loss:0.010924281930805586 | acc:0.7797308485378656\n",
            "epoch:3 | 4500/8313 batches| loss:0.010864601736001454 | acc:0.7806598533659187\n",
            "epoch:3 | 5000/8313 batches| loss:0.010839710881107838 | acc:0.7811531443711258\n",
            "epoch:3 | 5500/8313 batches| loss:0.010810195043252351 | acc:0.7816220914379204\n",
            "epoch:3 | 6000/8313 batches| loss:0.01077983475842266 | acc:0.7822263997667056\n",
            "epoch:3 | 6500/8313 batches| loss:0.010732486094391827 | acc:0.7833217966466698\n",
            "epoch:3 | 7000/8313 batches| loss:0.01070769220710848 | acc:0.783863465933438\n",
            "epoch:3 | 7500/8313 batches| loss:0.010679152170341443 | acc:0.7845078989468071\n",
            "epoch:3 | 8000/8313 batches| loss:0.010643057024925611 | acc:0.7852768403949506\n",
            "epoch:4 | 0/8313 batches| loss:0.015069385059177876 | acc:0.703125\n",
            "epoch:4 | 500/8313 batches| loss:0.010010612938002853 | acc:0.7976546906187625\n",
            "epoch:4 | 1000/8313 batches| loss:0.009886284365777398 | acc:0.8009178321678322\n",
            "epoch:4 | 1500/8313 batches| loss:0.009857047604847399 | acc:0.8014760992671552\n",
            "epoch:4 | 2000/8313 batches| loss:0.009824576712716585 | acc:0.802590892053973\n",
            "epoch:4 | 2500/8313 batches| loss:0.009825194959246853 | acc:0.8022353558576569\n",
            "epoch:4 | 3000/8313 batches| loss:0.009829685506117628 | acc:0.8022169693435521\n",
            "epoch:4 | 3500/8313 batches| loss:0.009826645084040415 | acc:0.802123500428449\n",
            "epoch:4 | 4000/8313 batches| loss:0.009796835154027746 | acc:0.8028149212696826\n",
            "epoch:4 | 4500/8313 batches| loss:0.0097786363161905 | acc:0.8030264107976005\n",
            "epoch:4 | 5000/8313 batches| loss:0.009770676570277236 | acc:0.8031768646270746\n",
            "epoch:4 | 5500/8313 batches| loss:0.00974115571992462 | acc:0.803734548263952\n",
            "epoch:4 | 6000/8313 batches| loss:0.009716121491508481 | acc:0.8041862814530911\n",
            "epoch:4 | 6500/8313 batches| loss:0.009695592542362156 | acc:0.8045589140132288\n",
            "epoch:4 | 7000/8313 batches| loss:0.009676927971448646 | acc:0.8050613305242108\n",
            "epoch:4 | 7500/8313 batches| loss:0.009647614231128025 | acc:0.8056155012664978\n",
            "epoch:4 | 8000/8313 batches| loss:0.009623402856652813 | acc:0.8061082208473941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "xVseoCJXkGuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*torch.compile(model, mode=\"reduce_overhead\")* allows our code to run more efficiently. However, this optimization may come at the cost of a small amount of additional memory. This is the recommended mode for small models like ours for sorting."
      ],
      "metadata": {
        "id": "6OzX1tUCkIem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yMTmYjAWkQzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DBpedia_label = {1: \"Company\",\n",
        "                 2: \"Educational Institution\",\n",
        "                 3: \"Artist\",\n",
        "                 4: \"Athlete\",\n",
        "                 5: \"OfficeHolder\",\n",
        "                 6: \"Mean of transportation\",\n",
        "                 7: \"Building\",\n",
        "                 8: \"Natural place\",\n",
        "                 9: \"Village\",\n",
        "                 10: \"Animal\",\n",
        "                 11: \"Plant\",\n",
        "                 12: \"Album\",\n",
        "                 13: \"Film\", \n",
        "                 14: \"Written Work\"}\n",
        "\n",
        "def predict(model: ClassificationModel, text: str) -> int:\n",
        "  with torch.no_grad():\n",
        "    text = torch.tensor(text_pipeline(text))\n",
        "    opt_mod = torch.compile(model, mode=\"reduce-overhead\")\n",
        "    output = opt_mod(text, torch.tensor([0]))\n",
        "    return output.argmax(1).item() + 1"
      ],
      "metadata": {
        "id": "Kbsfms7QiS5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Example_1 =\"Nithari is a village in the western part of the state of Uttar Pradesh \\\n",
        "            India bordering on New Delhi. Nithari forms part of the New Okhla Industrial \\\n",
        "            Development Authority's planned industrial city Noida falling in Sector 31. \\\n",
        "            Nithari made international news headlines in December 2006 when the skeletons\\\n",
        "             of a number of apparently murdered women and children were unearthed in the village.\"\n",
        "\n",
        "model_test = model_cm.to(\"cpu\")\n",
        "print(f\"Output example 1: {DBpedia_label[predict(model_test, Example_1)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhUkf4-pk5H2",
        "outputId": "def20db2-e72b-4d92-9d2e-83de788ca72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output example 1: Village\n"
          ]
        }
      ]
    }
  ]
}